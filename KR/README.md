![Static Badge](https://img.shields.io/badge/hadoop-3.3.6-%2366CCFF?style=flat-square&logo=apachehadoop&labelColor=white)
![Static Badge](https://img.shields.io/badge/OpenJDK-1.8.0-%23437291?style=flat-square&logo=openjdk&labelColor=white)
![Static Badge](https://img.shields.io/badge/VirtualBox-7.0.10-%23183A61?style=flat-square&logo=virtualbox&logoColor=%23183A61&labelColor=white)
![Static Badge](https://img.shields.io/badge/Ubuntu-22.04-%23E95420?style=flat-square&logo=ubuntu&labelColor=white)

# ***í•˜ë‘¡ ì„¤ì¹˜ ê°€ì´ë“œ***
ì´ ë ˆí¬ì§€í† ë¦¬ëŠ” `í•˜ë‘¡ ì—ì½” ì‹œìŠ¤í…œ`ì„ ì˜¤ë¼í´ ë²„ì¸„ì–´ ë¨¸ì‹ ì— ì„¤ì¹˜í•˜ëŠ” ë°©ë²•ì„ ê¸°ìˆ í•´ ë†“ì€ ê²ƒ ì…ë‹ˆë‹¤.

## __ëª©ì°¨__
### 1. ***[VM ì„¤ì •](#setting)***
### 2. ***[í•˜ë‘¡ ë‹¤ìš´ë¡œë“œ ë° ì„¤ì •](#install)***
### 3. ***[í•˜ë‘¡ ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸](#test)***
### 4. ***[ìŠ¤íŒŒí¬ ë‹¤ìš´ë¡œë“œ ë° ì„¤ì •](#spark)***



<a name='setting'></a>

## 1. VM ì„¤ì •

ì €í¬ëŠ” 4ê°œì˜ ë²„ì¸„ì–¼ ë¨¸ì‹ ì„ ë§Œë“¤ê²ƒ ì…ë‹ˆë‹¤.

```yml
VM ìŠ¤íŒ©.

master-node
    - Processor : 2
    - System memory : 4096 MB
    - Vedio memory : 16 MB

worker-node1~3
    - Processor : 2
    - System memory : 4096 MB
    - Vedio memory : 16 MB
```

í”„ë¡œì„¸ìŠ¤ë¥¼ 2ì½”ì–´ë¡œ ì„¤ì •í•´ì„œ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” ë‚˜ì¤‘ì— ì¿ ë²„ë„¤í‹°ìŠ¤ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œ ì…ë‹ˆë‹¤.

ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì • í•˜ì…”ë„ ë¬¸ì œ ì—†ìŠµë‹ˆë‹¤.

ì´ì œ ê³ ì • IPë¥¼ ê° ê°€ìƒë¨¸ì‹ ì— ì„¤ì •í•´ ì¤ë‹ˆë‹¤.

ë‹¤ìŒê³¼ ê°™ì€ ê³ ì • IPë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.
```yml
master-node
    - IP : 192.168.1.10
    - Port : 10
worker-node1 
    - IP : 192.168.1.11
    - Port : 11
worker-node2 
    - IP : 192.168.1.12
    - Port : 12
worker-node3
    - IP : 192.168.1.13
    - Port : 13
```

ë„¤íŠ¸ì›Œí¬ ì„¤ì • íŒŒì¼ì„ ìˆ˜ì •í•˜ê¸° ìœ„í•œ ê²½ë¡œì…ë‹ˆë‹¤.

```bash
$ sudo vi /etc/netplan/00-installer-config.yaml
```

êµ¬ì„± íŒŒì¼ì„ ì—´ê³  ê° ë…¸ë“œì˜ IP ê°’ì„ ì…ë ¥í•˜ì‹­ì‹œì˜¤. ë‹¤ìŒì€ ë§ˆìŠ¤í„° ë…¸ë“œ ì„¤ì • ì˜ˆì‹œì…ë‹ˆë‹¤.

```yaml
# This is the network config written by 'subiquity'
network:
    ethernets:
        enp0s3:
            addresses:
                - 192.168.1.10/24
            nameservers: 
                addresses: [8.8.8.8, 8.8.4.4]
            routes:
                - to: default
                  via: 192.168.1.1
    version: 2
```

```bash
$ sudo netplan apply
```
ë‚˜ë¨¸ì§€ ê°€ìƒ ë¨¸ì‹ ì— ëŒ€í•œ ì„¤ì •ì„ ì™„ë£Œí•˜ê³  ë³€ê²½ ì‚¬í•­ì„ ì ìš©í•©ë‹ˆë‹¤.

`ifconfig` ëª…ë ¹ì–´ë¥¼ í†µí•´ ë³€ê²½ëœ IPë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ëª¨ë“  ë…¸ë“œë“¤ì˜ í˜¸ìŠ¤íŠ¸ë¥¼ ì„¤ì •í•´ ì¤ë‹ˆë‹¤.
```bash
$ sudo vi /etc/hosts
```
```vim
127.0.0.1 localhost
192.168.0.10 master-node
192.168.0.11 worker-node1
192.168.0.12 worker-node2
192.168.0.13 worker-node3
...
```

ë‹¤ìŒìœ¼ë¡œ, Open JDKì™€ Python3ë¥¼ ë‹¤ìš´ë¡œë“œ í•´ì¤ë‹ˆë‹¤.

```bash
$ sudo apt-get update
$ sudo apt-get install openjdk-8-jdk
$ sudo apt install python3-pip
```

ì›Œì»¤ ë…¸ë“œë“¤ì˜ ì´ë¦„ì„ ë°”ê¾¸ê³  ì ìš©í•´ ì¤ë‹ˆë‹¤. ( worker-node1~3 )
```bash
$ sudo vi /etc/hostname
```

```vim
worker-node1
```

```bash
$ sudo hostname -F /etc/hostname # all-node
$ sudo hostname apply
$ sudo reboot
```

`sshd_config` íŒŒì¼ì˜ PubkeyAuthentication ë¶€ë¶„ ì£¼ì„ì„ ì œê±°í•´ ì¤ë‹ˆë‹¤.

```bash
$ sudo vi /etc/ssh/sshd_config
```

```bash
...
PubkeyAuthentication yes
...
```

í•˜ë‘¡ ë§ˆìŠ¤í„° ë…¸ë“œëŠ” ëª¨ë“  ì›Œì»¤ ë…¸ë“œì— ëŒ€í•´ ë¹„ë°€ë²ˆí˜¸ ì—†ì´ SSHë¥¼ ì ‘ì†í•  ìˆ˜ ìˆë„ë¡ í™œì„±í™”í•©ë‹ˆë‹¤.

```bash
$ chmod 700 ~/.ssh
$ ssh-keygen -t rsa -P ""
# Enter
```

ë§ˆìŠ¤í„° ë…¸ë“œì˜ ssh keyë¥¼ ì›Œì»¤ë…¸ë“œë“¤ì— ë³µì‚¬í•´ ì¤ë‹ˆë‹¤.

```bash
# master node only
$ ssh-copy-id -i ~/.ssh/id_rsa.pub master-node
$ ssh-copy-id -i ~/.ssh/id_rsa.pub worker-node1
$ ssh-copy-id -i ~/.ssh/id_rsa.pub worker-node2
$ ssh-copy-id -i ~/.ssh/id_rsa.pub worker-node3

# Check if ssh is connected properly
$ ssh worker-node1
```

<a name='install'></a>

## 2. í•˜ë‘¡ ë‹¤ìš´ë¡œë“œ ë° ì„¤ì •

ëª¨ë“  ë…¸ë“œì— í•˜ë‘¡ì„ ë‹¤ìš´ë¡œë“œ í•´ì¤ë‹ˆë‹¤.

```bash
$ wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
$ tar -xvf hadoop-3.3.6.tar.gz
$ mv hadoop-3.3.6 hadoop
```

ì••ì¶•ì„ í‘¼ í•˜ë‘¡ íŒŒì¼ì•ˆì— ìˆëŠ” workersíŒŒì¼ì— ë‹¤ìŒê³¼ ê°™ì´ ì ì–´ì¤ë‹ˆë‹¤.

```bash
$ vi hadoop/etc/hadoop/workers
# Workers File
worker-node1
worker-node2
worker-node3
# :wq!

$ vi hadoop/etc/hadoop/masters
# Masters File
master-node
# :wq!
```

ì´ì œ í•˜ë‘¡ì„ ìš´ì˜í•˜ê¸° ìœ„í•œ ê¸°ë³¸ì ì¸ í”„ë ˆì„ì„ ìƒì„±í•´ ì¤ë‹ˆë‹¤.

`ì£¼ì˜ : namenodeëŠ” ì˜¤ì§ ë§ˆìŠ¤í„° ë…¸ë“œì—ë§Œ ìƒì„±í•´ ì¤˜ì•¼ í•©ë‹ˆë‹¤.`

```bash
# master node
$ mkdir data
$ cd data
$ mkdir namenode datanode tmp userlogs

# worker nodes
$ mkdir data
$ cd data
$ mkdir datanode tmp userlogs
```

`~/.bashrc` íŒŒì¼ ëì— ê²½ë¡œë¥¼ ì§€ì •í•˜ì„¸ìš”.

```bash
$ sudo vi ~/.bashrc

# .bashrc
...
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HADOOP_HOME=/home/master-node/hadoop
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export HADOOP_CLASSPATH=${JAVA_HOME}/lib/tools.jar
# :wq!

$ source ~/.bashrc
```

`hadoop-env.sh` íŒŒì¼ì— ë‹¤ìŒê³¼ ê°™ì€ ë‚´ìš©ì„ ì¶”ê°€í•´ ì£¼ì„¸ìš”.

```bash
$ vi hadoop/etc/hadoop/hadoop-env.sh

# hadoop-env.sh
...
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
# :wq!
```
`core-site.xml`, `yarn-site.xml`, `hdfs-site.xml` íŒŒì¼ë“¤ì„ ì„¤ì •í•´ ì£¼ì„¸ìš”.

- core-site.xml
```xml
<!-- all node are the same -->
...
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://master-node:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/master-node/data/tmp</value>
    </property>
</configration>
```

- yarn-site.xml 

```xml
<!-- master node -->
...
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>master-node</value>
    </property>
</configration>
```

```xml
<!-- worker nodes -->
...
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
</configration>
```

- hdfs-site.xml
```xml
<!-- master node -->
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/home/master-node/data/namenode</value>
    </property>
</configration>
```

```xml
<!-- worker nodes -->
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/home/master-node/data/datanode</value>
    </property>
</configration>
```


<a name='test'></a>

## 3. í•˜ë‘¡ ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸

ë“œë””ì–´ ë§ˆì§€ë§‰ ìŠ¤í…ì…ë‹ˆë‹¤!!

namenodeë¥¼ ì´ˆê¸°í™” í•´ì¤ë‹ˆë‹¤.

```bash
# master node
$ hdfs namenode -format
$ start-all.sh

# Check if this works!
$ jps

# If it works correctly, you will see the following command line:
1380 SecondaryNameNode
2391 Jps
1480 ResourceManager
1840 NameNode

# The same behavior can be seen on worker nodes.
4801 DataNode
4248 Jps
3059 NodeManager
```

### ***ë“œë””ì–´ ìš°ë¦¬ëŠ” í•˜ë‘¡ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!!*** ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ 
```bash
$ hdfs dfs -mkdir /test
$ hdfs dfs -ls /
# Check your COMMEND!!!
```

Next Step is install `SPARK`!


## Not Yet
<a name='spark'></a>

## 4. Install SPARK

```bash
$ wget https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz
$ tar -xvf spark-3.5.0-bin-hadoop3.tgz
$ mv spark-3.5.0-bin-hadoop3 spark
```

`~/.bashrc` file.
```bash
...
export SPARK_HOME=/home/master-node/spark
export PYTHONPATH=$SPARK_HOME/python/lib/pyspark.zip:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip
export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:$LD_LIBRARY_PATH
```

```bash
# master-node
$ cp spark/conf/spark-defaults.conf.template spark/conf/spark-defaults.conf
$ vi spark/conf/spark-defaults.conf
```
```bash
# master-node
...
spark.master                     yarn
spark.eventLog.enabled           true
spark.eventLog.dir               file://home/master-node/spark/sparkeventlog
spark.serializer                 org.apache.spark.serializer.KryoSerializer
spark.driver.memory              512m
spark.yarn.am.memory             256m
```

```bash
# master-node
$ mkdir spark/sparkeventlog
$ sudo ln -s /usr/bin/python3 /usr/bin/python
```
